{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINGER TAPPING ANALYSIS\n",
    "##### Data collected from patients with neurodegenerative disorders as well as healthy controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vladislava\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy import stats\n",
    "import math\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "import keras\n",
    "from keras.models import Model, model_from_yaml\n",
    "from keras.layers import Input, Conv1D, Flatten, Dropout, MaxPooling1D, Dense\n",
    "from keras.layers import Activation, BatchNormalization, concatenate\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "%matplotlib inline\n",
    "\n",
    "# from IPython.display import Audio\n",
    "# sound_file = './sound/beep.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "\n",
    "train = scipy.io.loadmat('TrainData.mat')\n",
    "val = scipy.io.loadmat('ValData.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain = train['X'], train['Y']\n",
    "Xval, Yval = val['X'], val['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arite, lez make a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNModel(inputShape, nConvLayers):\n",
    "    \n",
    "    input1 = Input(shape = inputShape)\n",
    "\n",
    "    #convolutions\n",
    "    x = input1\n",
    "    \n",
    "    \n",
    "    for i in range(0,nConvLayers):\n",
    "        \n",
    "        nFilters = 128 if i>1 else 32*(i+1)\n",
    "        inside = 3 if i>1 else 2\n",
    "        for temp in range(0,inside):\n",
    "            x = Conv1D(filters = nFilters,\n",
    "                  kernel_size = 5,\n",
    "                  padding = 'same',\n",
    "                  strides = 1,\n",
    "                  name = 'Conv1x5{}{}'.format(i,temp))(x)\n",
    "            \n",
    "            x = Activation('relu',name='ReLu{}{}'.format(i,temp))(x)\n",
    "            x = BatchNormalization()(x)\n",
    "            \n",
    "        x = MaxPooling1D(2,\n",
    "                      padding = 'same',\n",
    "                      strides = 1,\n",
    "                      name = 'MaxPooling1D{}'.format(i))(x)\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Fully connected\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(128)(x)\n",
    "    x = Activation('relu', name = 'reLU_dense')(x)\n",
    "    x = Dropout(0.6)(x)\n",
    "    \n",
    "    x = Dense(4)(x)\n",
    "    x = Activation('softmax',name = 'Softmax')(x)\n",
    "\n",
    "    m = Model(input1,x)\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModelTopology(model,modelName):\n",
    "    model_json = model.to_yaml()\n",
    "    with open(modelName+'.yaml', \"w\") as yaml_file:\n",
    "        yaml_file.write(model_json)\n",
    "    print(\"Saved model to {}.yaml\".format(modelName))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defCallbacks(weightFile):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(weightFile,\n",
    "                                 monitor='val_acc',\n",
    "                                 verbose=1,\n",
    "                                 save_best_only = True,\n",
    "                                 save_weights_only = True,\n",
    "                                 mode='max')\n",
    "    early = EarlyStopping(monitor='val_acc',\n",
    "                          patience = 15,\n",
    "                          verbose = 1,\n",
    "                          mode='max')\n",
    "    return [checkpoint, early]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitModel(model, modelName, Xtrain, Ytrain, Xval, Yval, epochs,batch_size):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    # make file name\n",
    "    tm = time.gmtime()\n",
    "    weightFile = 'BEST_WEIGHTS{}.{}.{}.{}.{}.{}.h5'.format(modelName,tm[2],tm[1],tm[0],tm[3]+1,tm[4])\n",
    "    \n",
    "    #define callbacks\n",
    "    callbacks = defCallbacks(weightFile)\n",
    "    \n",
    "    # FIT THE MODEL\n",
    "    history = model.fit(x = Xtrain, y = Ytrain,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data = (Xval,Yval),\n",
    "                        callbacks = callbacks)\n",
    "    toc = time.time()\n",
    "    print(\"Finished training in {} min ({} h)\".format(round((toc-tic)/60,2),round((toc-tic)/3600,2)))\n",
    "\n",
    "    \n",
    "    # Save the weights\n",
    "    #model.save_weights(str(modelName)+'.h5') # ???????\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluateModel(model, modelName, Xval, Yval):\n",
    "#     tic = time.time()\n",
    "#     predictions = model.predict(Xval)\n",
    "#     toc = time.time()\n",
    "#     print('Finished prediction in: {} min'.format(round((toc-tic)/60,2)))\n",
    "\n",
    "#     print('Evaluating...')\n",
    "#     score = model.evaluate(Xval,Yval,verbose=1)\n",
    "#     print(score)\n",
    "\n",
    "#     #Save that stuff too pls.\n",
    "#     tm = time.gmtime()    \n",
    "#     predictionFile = 'Predictions-{}.{}.{}.{}.{}.{}.csv'.format(modelName,tm[2],tm[1],tm[0],tm[3]+1,tm[4])\n",
    "\n",
    "#     dfPredicted = pd.DataFrame(predictions)\n",
    "#     dfPredicted = dfPredicted.idxmax(axis =1)\n",
    "#     dfExpected = pd.DataFrame(Yval)\n",
    "#     dfExpected = dfExpected.idxmax(axis =1)\n",
    "#     df = pd.DataFrame({'Predicted':dfPredicted, 'Expected':dfExpected})\n",
    "#     df.to_csv(predictionFile,index = False)\n",
    "#     print('Saved predictions to: ', predictionFile)\n",
    "    \n",
    "#     bingos = sum(df['Predicted'] ==df['Expected'])\n",
    "#     accRly = 100*bingos/df.shape[0]\n",
    "#     print('Currently your actual accuracy on Xval is: {}%'.format(round(accRly,2)))\n",
    "    \n",
    "#     return accRly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 2000, 6)           0         \n",
      "_________________________________________________________________\n",
      "Conv1x500 (Conv1D)           (None, 2000, 32)          992       \n",
      "_________________________________________________________________\n",
      "ReLu00 (Activation)          (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2000, 32)          128       \n",
      "_________________________________________________________________\n",
      "Conv1x501 (Conv1D)           (None, 2000, 32)          5152      \n",
      "_________________________________________________________________\n",
      "ReLu01 (Activation)          (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2000, 32)          128       \n",
      "_________________________________________________________________\n",
      "MaxPooling1D0 (MaxPooling1D) (None, 2000, 32)          0         \n",
      "_________________________________________________________________\n",
      "Conv1x510 (Conv1D)           (None, 2000, 64)          10304     \n",
      "_________________________________________________________________\n",
      "ReLu10 (Activation)          (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "Conv1x511 (Conv1D)           (None, 2000, 64)          20544     \n",
      "_________________________________________________________________\n",
      "ReLu11 (Activation)          (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2000, 64)          256       \n",
      "_________________________________________________________________\n",
      "MaxPooling1D1 (MaxPooling1D) (None, 2000, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv1x520 (Conv1D)           (None, 2000, 128)         41088     \n",
      "_________________________________________________________________\n",
      "ReLu20 (Activation)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "Conv1x521 (Conv1D)           (None, 2000, 128)         82048     \n",
      "_________________________________________________________________\n",
      "ReLu21 (Activation)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "Conv1x522 (Conv1D)           (None, 2000, 128)         82048     \n",
      "_________________________________________________________________\n",
      "ReLu22 (Activation)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "MaxPooling1D2 (MaxPooling1D) (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv1x530 (Conv1D)           (None, 2000, 128)         82048     \n",
      "_________________________________________________________________\n",
      "ReLu30 (Activation)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "Conv1x531 (Conv1D)           (None, 2000, 128)         82048     \n",
      "_________________________________________________________________\n",
      "ReLu31 (Activation)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "Conv1x532 (Conv1D)           (None, 2000, 128)         82048     \n",
      "_________________________________________________________________\n",
      "ReLu32 (Activation)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 2000, 128)         512       \n",
      "_________________________________________________________________\n",
      "MaxPooling1D3 (MaxPooling1D) (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32768128  \n",
      "_________________________________________________________________\n",
      "reLU_dense (Activation)      (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "Softmax (Activation)         (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 33,260,804\n",
      "Trainable params: 33,258,884\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "Saved model to CNN0550554.yaml\n",
      "TRAINING...\n",
      "Train on 2719 samples, validate on 375 samples\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2719/2719 [==============================] - ETA: 11:30 - loss: 4.5776 - acc: 0.18 - ETA: 5:57 - loss: 4.3125 - acc: 0.1562 - ETA: 4:07 - loss: 4.3994 - acc: 0.166 - ETA: 3:11 - loss: 4.0995 - acc: 0.218 - ETA: 2:38 - loss: 3.8852 - acc: 0.225 - ETA: 2:15 - loss: 4.1439 - acc: 0.218 - ETA: 1:59 - loss: 4.0369 - acc: 0.223 - ETA: 1:47 - loss: 3.9445 - acc: 0.210 - ETA: 1:38 - loss: 3.9976 - acc: 0.215 - ETA: 1:30 - loss: 3.8832 - acc: 0.225 - ETA: 1:24 - loss: 3.7997 - acc: 0.233 - ETA: 1:19 - loss: 3.7496 - acc: 0.229 - ETA: 1:15 - loss: 3.6394 - acc: 0.245 - ETA: 1:11 - loss: 3.5025 - acc: 0.267 - ETA: 1:07 - loss: 3.5164 - acc: 0.266 - ETA: 1:04 - loss: 3.4483 - acc: 0.269 - ETA: 1:02 - loss: 3.3650 - acc: 0.272 - ETA: 59s - loss: 3.3364 - acc: 0.267 - ETA: 57s - loss: 3.3005 - acc: 0.27 - ETA: 55s - loss: 3.2785 - acc: 0.27 - ETA: 54s - loss: 3.2415 - acc: 0.27 - ETA: 52s - loss: 3.1765 - acc: 0.28 - ETA: 51s - loss: 3.0929 - acc: 0.29 - ETA: 49s - loss: 3.0951 - acc: 0.29 - ETA: 48s - loss: 3.0516 - acc: 0.29 - ETA: 47s - loss: 3.0280 - acc: 0.29 - ETA: 46s - loss: 3.0040 - acc: 0.29 - ETA: 45s - loss: 3.0453 - acc: 0.29 - ETA: 44s - loss: 3.0083 - acc: 0.29 - ETA: 43s - loss: 2.9918 - acc: 0.29 - ETA: 42s - loss: 2.9744 - acc: 0.29 - ETA: 41s - loss: 2.9494 - acc: 0.29 - ETA: 40s - loss: 2.9216 - acc: 0.29 - ETA: 39s - loss: 2.8678 - acc: 0.30 - ETA: 39s - loss: 2.8337 - acc: 0.31 - ETA: 38s - loss: 2.8431 - acc: 0.30 - ETA: 37s - loss: 2.8139 - acc: 0.30 - ETA: 36s - loss: 2.7847 - acc: 0.31 - ETA: 36s - loss: 2.7457 - acc: 0.31 - ETA: 35s - loss: 2.7445 - acc: 0.31 - ETA: 35s - loss: 2.7300 - acc: 0.31 - ETA: 34s - loss: 2.7184 - acc: 0.31 - ETA: 34s - loss: 2.6941 - acc: 0.31 - ETA: 33s - loss: 2.6719 - acc: 0.31 - ETA: 32s - loss: 2.6539 - acc: 0.31 - ETA: 32s - loss: 2.6283 - acc: 0.31 - ETA: 31s - loss: 2.6130 - acc: 0.31 - ETA: 31s - loss: 2.5852 - acc: 0.32 - ETA: 31s - loss: 2.5684 - acc: 0.32 - ETA: 30s - loss: 2.5620 - acc: 0.31 - ETA: 30s - loss: 2.5618 - acc: 0.31 - ETA: 29s - loss: 2.5371 - acc: 0.31 - ETA: 29s - loss: 2.5416 - acc: 0.31 - ETA: 28s - loss: 2.5201 - acc: 0.31 - ETA: 28s - loss: 2.5092 - acc: 0.31 - ETA: 28s - loss: 2.4973 - acc: 0.31 - ETA: 27s - loss: 2.4764 - acc: 0.31 - ETA: 27s - loss: 2.4580 - acc: 0.31 - ETA: 27s - loss: 2.4424 - acc: 0.31 - ETA: 26s - loss: 2.4283 - acc: 0.31 - ETA: 26s - loss: 2.4126 - acc: 0.31 - ETA: 25s - loss: 2.3969 - acc: 0.32 - ETA: 25s - loss: 2.3809 - acc: 0.32 - ETA: 25s - loss: 2.3654 - acc: 0.32 - ETA: 24s - loss: 2.3556 - acc: 0.32 - ETA: 24s - loss: 2.3456 - acc: 0.32 - ETA: 24s - loss: 2.3299 - acc: 0.32 - ETA: 23s - loss: 2.3159 - acc: 0.33 - ETA: 23s - loss: 2.3063 - acc: 0.32 - ETA: 23s - loss: 2.2959 - acc: 0.32 - ETA: 22s - loss: 2.2850 - acc: 0.32 - ETA: 22s - loss: 2.2711 - acc: 0.33 - ETA: 22s - loss: 2.2563 - acc: 0.33 - ETA: 22s - loss: 2.2438 - acc: 0.33 - ETA: 21s - loss: 2.2332 - acc: 0.33 - ETA: 21s - loss: 2.2211 - acc: 0.33 - ETA: 21s - loss: 2.2114 - acc: 0.33 - ETA: 20s - loss: 2.2098 - acc: 0.33 - ETA: 20s - loss: 2.2018 - acc: 0.33 - ETA: 20s - loss: 2.1979 - acc: 0.33 - ETA: 20s - loss: 2.1905 - acc: 0.33 - ETA: 19s - loss: 2.1850 - acc: 0.33 - ETA: 19s - loss: 2.1749 - acc: 0.33 - ETA: 19s - loss: 2.1617 - acc: 0.33 - ETA: 18s - loss: 2.1477 - acc: 0.33 - ETA: 18s - loss: 2.1436 - acc: 0.33 - ETA: 18s - loss: 2.1335 - acc: 0.33 - ETA: 18s - loss: 2.1276 - acc: 0.33 - ETA: 17s - loss: 2.1247 - acc: 0.33 - ETA: 17s - loss: 2.1181 - acc: 0.33 - ETA: 17s - loss: 2.1126 - acc: 0.33 - ETA: 17s - loss: 2.1074 - acc: 0.33 - ETA: 16s - loss: 2.1023 - acc: 0.33 - ETA: 16s - loss: 2.0935 - acc: 0.33 - ETA: 16s - loss: 2.0854 - acc: 0.33 - ETA: 16s - loss: 2.0765 - acc: 0.33 - ETA: 15s - loss: 2.0709 - acc: 0.33 - ETA: 15s - loss: 2.0629 - acc: 0.33 - ETA: 15s - loss: 2.0594 - acc: 0.33 - ETA: 15s - loss: 2.0499 - acc: 0.33 - ETA: 14s - loss: 2.0465 - acc: 0.33 - ETA: 14s - loss: 2.0399 - acc: 0.33 - ETA: 14s - loss: 2.0324 - acc: 0.33 - ETA: 14s - loss: 2.0385 - acc: 0.33 - ETA: 13s - loss: 2.0352 - acc: 0.33 - ETA: 13s - loss: 2.0257 - acc: 0.33 - ETA: 13s - loss: 2.0248 - acc: 0.33 - ETA: 13s - loss: 2.0189 - acc: 0.33 - ETA: 12s - loss: 2.0158 - acc: 0.33 - ETA: 12s - loss: 2.0066 - acc: 0.33 - ETA: 12s - loss: 1.9995 - acc: 0.34 - ETA: 12s - loss: 1.9946 - acc: 0.34 - ETA: 12s - loss: 1.9883 - acc: 0.34 - ETA: 11s - loss: 1.9819 - acc: 0.34 - ETA: 11s - loss: 1.9743 - acc: 0.34 - ETA: 11s - loss: 1.9697 - acc: 0.34 - ETA: 11s - loss: 1.9624 - acc: 0.34 - ETA: 10s - loss: 1.9576 - acc: 0.34 - ETA: 10s - loss: 1.9515 - acc: 0.34 - ETA: 10s - loss: 1.9446 - acc: 0.34 - ETA: 10s - loss: 1.9390 - acc: 0.34 - ETA: 10s - loss: 1.9355 - acc: 0.34 - ETA: 9s - loss: 1.9298 - acc: 0.3445 - ETA: 9s - loss: 1.9250 - acc: 0.344 - ETA: 9s - loss: 1.9180 - acc: 0.346 - ETA: 9s - loss: 1.9120 - acc: 0.347 - ETA: 8s - loss: 1.9068 - acc: 0.349 - ETA: 8s - loss: 1.9044 - acc: 0.350 - ETA: 8s - loss: 1.9020 - acc: 0.348 - ETA: 8s - loss: 1.8971 - acc: 0.349 - ETA: 8s - loss: 1.8929 - acc: 0.347 - ETA: 7s - loss: 1.8887 - acc: 0.347 - ETA: 7s - loss: 1.8827 - acc: 0.349 - ETA: 7s - loss: 1.8795 - acc: 0.349 - ETA: 7s - loss: 1.8769 - acc: 0.349 - ETA: 6s - loss: 1.8750 - acc: 0.348 - ETA: 6s - loss: 1.8715 - acc: 0.347 - ETA: 6s - loss: 1.8680 - acc: 0.347 - ETA: 6s - loss: 1.8643 - acc: 0.348 - ETA: 6s - loss: 1.8580 - acc: 0.350 - ETA: 5s - loss: 1.8560 - acc: 0.349 - ETA: 5s - loss: 1.8509 - acc: 0.349 - ETA: 5s - loss: 1.8482 - acc: 0.351 - ETA: 5s - loss: 1.8454 - acc: 0.351 - ETA: 5s - loss: 1.8459 - acc: 0.350 - ETA: 4s - loss: 1.8419 - acc: 0.351 - ETA: 4s - loss: 1.8376 - acc: 0.352 - ETA: 4s - loss: 1.8342 - acc: 0.351 - ETA: 4s - loss: 1.8306 - acc: 0.351 - ETA: 4s - loss: 1.8284 - acc: 0.350 - ETA: 3s - loss: 1.8244 - acc: 0.351 - ETA: 3s - loss: 1.8204 - acc: 0.352 - ETA: 3s - loss: 1.8213 - acc: 0.351 - ETA: 3s - loss: 1.8196 - acc: 0.351 - ETA: 3s - loss: 1.8149 - acc: 0.352 - ETA: 2s - loss: 1.8114 - acc: 0.353 - ETA: 2s - loss: 1.8051 - acc: 0.353 - ETA: 2s - loss: 1.8026 - acc: 0.353 - ETA: 2s - loss: 1.8007 - acc: 0.352 - ETA: 1s - loss: 1.7959 - acc: 0.352 - ETA: 1s - loss: 1.7931 - acc: 0.352 - ETA: 1s - loss: 1.7883 - acc: 0.353 - ETA: 1s - loss: 1.7852 - acc: 0.354 - ETA: 1s - loss: 1.7818 - acc: 0.354 - ETA: 0s - loss: 1.7778 - acc: 0.356 - ETA: 0s - loss: 1.7744 - acc: 0.356 - ETA: 0s - loss: 1.7728 - acc: 0.356 - ETA: 0s - loss: 1.7720 - acc: 0.356 - ETA: 0s - loss: 1.7688 - acc: 0.357 - 36s 13ms/step - loss: 1.7671 - acc: 0.3575 - val_loss: 1.1710 - val_acc: 0.4480\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.44800, saving model to BEST_WEIGHTSCNN0550554.5.10.2018.9.34.h5\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2719/2719 [==============================] - ETA: 35s - loss: 0.7859 - acc: 0.62 - ETA: 33s - loss: 0.9195 - acc: 0.62 - ETA: 32s - loss: 1.0237 - acc: 0.56 - ETA: 31s - loss: 1.0275 - acc: 0.59 - ETA: 30s - loss: 1.0826 - acc: 0.55 - ETA: 30s - loss: 1.0986 - acc: 0.56 - ETA: 29s - loss: 1.1050 - acc: 0.56 - ETA: 29s - loss: 1.0923 - acc: 0.54 - ETA: 29s - loss: 1.0828 - acc: 0.54 - ETA: 29s - loss: 1.0910 - acc: 0.53 - ETA: 28s - loss: 1.0784 - acc: 0.53 - ETA: 28s - loss: 1.0715 - acc: 0.54 - ETA: 28s - loss: 1.1044 - acc: 0.52 - ETA: 27s - loss: 1.1043 - acc: 0.53 - ETA: 27s - loss: 1.0959 - acc: 0.53 - ETA: 27s - loss: 1.0786 - acc: 0.53 - ETA: 27s - loss: 1.0770 - acc: 0.54 - ETA: 27s - loss: 1.0893 - acc: 0.53 - ETA: 27s - loss: 1.0927 - acc: 0.52 - ETA: 26s - loss: 1.0992 - acc: 0.51 - ETA: 26s - loss: 1.0971 - acc: 0.51 - ETA: 26s - loss: 1.1063 - acc: 0.50 - ETA: 26s - loss: 1.1205 - acc: 0.49 - ETA: 26s - loss: 1.1078 - acc: 0.51 - ETA: 25s - loss: 1.1078 - acc: 0.50 - ETA: 25s - loss: 1.1160 - acc: 0.50 - ETA: 25s - loss: 1.1150 - acc: 0.50 - ETA: 25s - loss: 1.1149 - acc: 0.50 - ETA: 25s - loss: 1.1116 - acc: 0.50 - ETA: 24s - loss: 1.1093 - acc: 0.50 - ETA: 24s - loss: 1.0985 - acc: 0.51 - ETA: 24s - loss: 1.1003 - acc: 0.51 - ETA: 24s - loss: 1.0917 - acc: 0.52 - ETA: 24s - loss: 1.0933 - acc: 0.52 - ETA: 24s - loss: 1.1006 - acc: 0.51 - ETA: 23s - loss: 1.1046 - acc: 0.51 - ETA: 23s - loss: 1.1101 - acc: 0.50 - ETA: 23s - loss: 1.1108 - acc: 0.50 - ETA: 23s - loss: 1.1055 - acc: 0.50 - ETA: 23s - loss: 1.1078 - acc: 0.50 - ETA: 22s - loss: 1.1089 - acc: 0.50 - ETA: 22s - loss: 1.1069 - acc: 0.50 - ETA: 22s - loss: 1.1081 - acc: 0.50 - ETA: 22s - loss: 1.1064 - acc: 0.50 - ETA: 22s - loss: 1.1058 - acc: 0.50 - ETA: 22s - loss: 1.1072 - acc: 0.49 - ETA: 21s - loss: 1.1036 - acc: 0.50 - ETA: 21s - loss: 1.1085 - acc: 0.50 - ETA: 21s - loss: 1.1129 - acc: 0.50 - ETA: 21s - loss: 1.1150 - acc: 0.50 - ETA: 21s - loss: 1.1128 - acc: 0.50 - ETA: 20s - loss: 1.1187 - acc: 0.50 - ETA: 20s - loss: 1.1150 - acc: 0.51 - ETA: 20s - loss: 1.1084 - acc: 0.51 - ETA: 20s - loss: 1.1146 - acc: 0.51 - ETA: 20s - loss: 1.1198 - acc: 0.51 - ETA: 20s - loss: 1.1139 - acc: 0.51 - ETA: 19s - loss: 1.1123 - acc: 0.51 - ETA: 19s - loss: 1.1213 - acc: 0.51 - ETA: 19s - loss: 1.1152 - acc: 0.51 - ETA: 19s - loss: 1.1121 - acc: 0.51 - ETA: 19s - loss: 1.1080 - acc: 0.51 - ETA: 19s - loss: 1.1039 - acc: 0.52 - ETA: 18s - loss: 1.1007 - acc: 0.52 - ETA: 18s - loss: 1.1039 - acc: 0.52 - ETA: 18s - loss: 1.1044 - acc: 0.51 - ETA: 18s - loss: 1.1040 - acc: 0.51 - ETA: 18s - loss: 1.0998 - acc: 0.52 - ETA: 17s - loss: 1.0954 - acc: 0.52 - ETA: 17s - loss: 1.0960 - acc: 0.52 - ETA: 17s - loss: 1.0970 - acc: 0.52 - ETA: 17s - loss: 1.0992 - acc: 0.52 - ETA: 17s - loss: 1.0962 - acc: 0.52 - ETA: 17s - loss: 1.0956 - acc: 0.52 - ETA: 16s - loss: 1.0946 - acc: 0.52 - ETA: 16s - loss: 1.0944 - acc: 0.52 - ETA: 16s - loss: 1.0925 - acc: 0.53 - ETA: 16s - loss: 1.0908 - acc: 0.53 - ETA: 16s - loss: 1.0930 - acc: 0.53 - ETA: 15s - loss: 1.0913 - acc: 0.53 - ETA: 15s - loss: 1.0911 - acc: 0.53 - ETA: 15s - loss: 1.0892 - acc: 0.53 - ETA: 15s - loss: 1.0887 - acc: 0.53 - ETA: 15s - loss: 1.0908 - acc: 0.53 - ETA: 15s - loss: 1.0922 - acc: 0.53 - ETA: 14s - loss: 1.0926 - acc: 0.53 - ETA: 14s - loss: 1.0918 - acc: 0.53 - ETA: 14s - loss: 1.0871 - acc: 0.53 - ETA: 14s - loss: 1.0860 - acc: 0.53 - ETA: 14s - loss: 1.0846 - acc: 0.53 - ETA: 14s - loss: 1.0845 - acc: 0.53 - ETA: 13s - loss: 1.0819 - acc: 0.53 - ETA: 13s - loss: 1.0809 - acc: 0.53 - ETA: 13s - loss: 1.0784 - acc: 0.53 - ETA: 13s - loss: 1.0739 - acc: 0.53 - ETA: 13s - loss: 1.0714 - acc: 0.53 - ETA: 12s - loss: 1.0681 - acc: 0.53 - ETA: 12s - loss: 1.0690 - acc: 0.53 - ETA: 12s - loss: 1.0716 - acc: 0.53 - ETA: 12s - loss: 1.0737 - acc: 0.53 - ETA: 12s - loss: 1.0759 - acc: 0.53 - ETA: 12s - loss: 1.0750 - acc: 0.53 - ETA: 11s - loss: 1.0739 - acc: 0.53 - ETA: 11s - loss: 1.0741 - acc: 0.53 - ETA: 11s - loss: 1.0732 - acc: 0.53 - ETA: 11s - loss: 1.0763 - acc: 0.53 - ETA: 11s - loss: 1.0755 - acc: 0.53 - ETA: 10s - loss: 1.0726 - acc: 0.53 - ETA: 10s - loss: 1.0729 - acc: 0.53 - ETA: 10s - loss: 1.0697 - acc: 0.53 - ETA: 10s - loss: 1.0704 - acc: 0.53 - ETA: 10s - loss: 1.0715 - acc: 0.53 - ETA: 10s - loss: 1.0699 - acc: 0.53 - ETA: 9s - loss: 1.0705 - acc: 0.5340 - ETA: 9s - loss: 1.0710 - acc: 0.534 - ETA: 9s - loss: 1.0699 - acc: 0.533 - ETA: 9s - loss: 1.0710 - acc: 0.533 - ETA: 9s - loss: 1.0707 - acc: 0.532 - ETA: 9s - loss: 1.0713 - acc: 0.532 - ETA: 8s - loss: 1.0704 - acc: 0.534 - ETA: 8s - loss: 1.0712 - acc: 0.535 - ETA: 8s - loss: 1.0725 - acc: 0.534 - ETA: 8s - loss: 1.0749 - acc: 0.534 - ETA: 8s - loss: 1.0739 - acc: 0.534 - ETA: 7s - loss: 1.0721 - acc: 0.535 - ETA: 7s - loss: 1.0721 - acc: 0.535 - ETA: 7s - loss: 1.0712 - acc: 0.535 - ETA: 7s - loss: 1.0690 - acc: 0.536 - ETA: 7s - loss: 1.0677 - acc: 0.537 - ETA: 7s - loss: 1.0645 - acc: 0.538 - ETA: 6s - loss: 1.0628 - acc: 0.538 - ETA: 6s - loss: 1.0624 - acc: 0.538 - ETA: 6s - loss: 1.0604 - acc: 0.539 - ETA: 6s - loss: 1.0600 - acc: 0.539 - ETA: 6s - loss: 1.0613 - acc: 0.540 - ETA: 6s - loss: 1.0621 - acc: 0.540 - ETA: 5s - loss: 1.0621 - acc: 0.540 - ETA: 5s - loss: 1.0618 - acc: 0.541 - ETA: 5s - loss: 1.0631 - acc: 0.540 - ETA: 5s - loss: 1.0617 - acc: 0.541 - ETA: 5s - loss: 1.0614 - acc: 0.542 - ETA: 4s - loss: 1.0625 - acc: 0.540 - ETA: 4s - loss: 1.0632 - acc: 0.540 - ETA: 4s - loss: 1.0619 - acc: 0.541 - ETA: 4s - loss: 1.0626 - acc: 0.543 - ETA: 4s - loss: 1.0638 - acc: 0.541 - ETA: 4s - loss: 1.0637 - acc: 0.541 - ETA: 3s - loss: 1.0638 - acc: 0.541 - ETA: 3s - loss: 1.0641 - acc: 0.541 - ETA: 3s - loss: 1.0631 - acc: 0.542 - ETA: 3s - loss: 1.0637 - acc: 0.542 - ETA: 3s - loss: 1.0621 - acc: 0.543 - ETA: 3s - loss: 1.0593 - acc: 0.544 - ETA: 2s - loss: 1.0584 - acc: 0.543 - ETA: 2s - loss: 1.0579 - acc: 0.543 - ETA: 2s - loss: 1.0581 - acc: 0.542 - ETA: 2s - loss: 1.0602 - acc: 0.541 - ETA: 2s - loss: 1.0598 - acc: 0.541 - ETA: 1s - loss: 1.0589 - acc: 0.542 - ETA: 1s - loss: 1.0566 - acc: 0.544 - ETA: 1s - loss: 1.0556 - acc: 0.544 - ETA: 1s - loss: 1.0530 - acc: 0.545 - ETA: 1s - loss: 1.0528 - acc: 0.545 - ETA: 1s - loss: 1.0525 - acc: 0.545 - ETA: 0s - loss: 1.0547 - acc: 0.544 - ETA: 0s - loss: 1.0546 - acc: 0.545 - ETA: 0s - loss: 1.0558 - acc: 0.543 - ETA: 0s - loss: 1.0550 - acc: 0.544 - ETA: 0s - loss: 1.0547 - acc: 0.544 - 31s 12ms/step - loss: 1.0549 - acc: 0.5439 - val_loss: 1.0280 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.44800 to 0.55467, saving model to BEST_WEIGHTSCNN0550554.5.10.2018.9.34.h5\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2719/2719 [==============================] - ETA: 33s - loss: 1.7019 - acc: 0.43 - ETA: 31s - loss: 1.2098 - acc: 0.62 - ETA: 30s - loss: 1.0425 - acc: 0.70 - ETA: 30s - loss: 1.0086 - acc: 0.68 - ETA: 30s - loss: 0.9581 - acc: 0.70 - ETA: 29s - loss: 0.9058 - acc: 0.70 - ETA: 29s - loss: 0.8657 - acc: 0.73 - ETA: 29s - loss: 0.8715 - acc: 0.70 - ETA: 29s - loss: 0.9081 - acc: 0.66 - ETA: 28s - loss: 0.9034 - acc: 0.65 - ETA: 28s - loss: 0.8990 - acc: 0.64 - ETA: 28s - loss: 0.8844 - acc: 0.65 - ETA: 28s - loss: 0.8490 - acc: 0.66 - ETA: 28s - loss: 0.8269 - acc: 0.67 - ETA: 27s - loss: 0.8494 - acc: 0.66 - ETA: 27s - loss: 0.8346 - acc: 0.66 - ETA: 27s - loss: 0.8423 - acc: 0.66 - ETA: 27s - loss: 0.8599 - acc: 0.65 - ETA: 27s - loss: 0.8586 - acc: 0.65 - ETA: 26s - loss: 0.8673 - acc: 0.64 - ETA: 26s - loss: 0.8687 - acc: 0.64 - ETA: 26s - loss: 0.8607 - acc: 0.64 - ETA: 26s - loss: 0.8765 - acc: 0.64 - ETA: 26s - loss: 0.8734 - acc: 0.64 - ETA: 25s - loss: 0.8697 - acc: 0.64 - ETA: 25s - loss: 0.8673 - acc: 0.64 - ETA: 25s - loss: 0.8590 - acc: 0.64 - ETA: 25s - loss: 0.8567 - acc: 0.64 - ETA: 25s - loss: 0.8503 - acc: 0.64 - ETA: 25s - loss: 0.8475 - acc: 0.64 - ETA: 24s - loss: 0.8490 - acc: 0.63 - ETA: 24s - loss: 0.8486 - acc: 0.63 - ETA: 24s - loss: 0.8429 - acc: 0.63 - ETA: 24s - loss: 0.8383 - acc: 0.63 - ETA: 24s - loss: 0.8421 - acc: 0.63 - ETA: 23s - loss: 0.8409 - acc: 0.63 - ETA: 23s - loss: 0.8464 - acc: 0.63 - ETA: 23s - loss: 0.8443 - acc: 0.63 - ETA: 23s - loss: 0.8373 - acc: 0.64 - ETA: 23s - loss: 0.8389 - acc: 0.63 - ETA: 23s - loss: 0.8408 - acc: 0.63 - ETA: 22s - loss: 0.8422 - acc: 0.63 - ETA: 22s - loss: 0.8472 - acc: 0.62 - ETA: 22s - loss: 0.8454 - acc: 0.63 - ETA: 22s - loss: 0.8434 - acc: 0.63 - ETA: 22s - loss: 0.8430 - acc: 0.63 - ETA: 21s - loss: 0.8413 - acc: 0.63 - ETA: 21s - loss: 0.8369 - acc: 0.63 - ETA: 21s - loss: 0.8467 - acc: 0.63 - ETA: 21s - loss: 0.8476 - acc: 0.63 - ETA: 21s - loss: 0.8457 - acc: 0.63 - ETA: 21s - loss: 0.8481 - acc: 0.63 - ETA: 20s - loss: 0.8461 - acc: 0.63 - ETA: 20s - loss: 0.8419 - acc: 0.63 - ETA: 20s - loss: 0.8448 - acc: 0.63 - ETA: 20s - loss: 0.8432 - acc: 0.63 - ETA: 20s - loss: 0.8459 - acc: 0.63 - ETA: 19s - loss: 0.8371 - acc: 0.63 - ETA: 19s - loss: 0.8391 - acc: 0.63 - ETA: 19s - loss: 0.8358 - acc: 0.63 - ETA: 19s - loss: 0.8352 - acc: 0.63 - ETA: 19s - loss: 0.8281 - acc: 0.64 - ETA: 19s - loss: 0.8254 - acc: 0.64 - ETA: 18s - loss: 0.8295 - acc: 0.63 - ETA: 18s - loss: 0.8280 - acc: 0.63 - ETA: 18s - loss: 0.8281 - acc: 0.63 - ETA: 18s - loss: 0.8261 - acc: 0.63 - ETA: 18s - loss: 0.8258 - acc: 0.63 - ETA: 17s - loss: 0.8247 - acc: 0.64 - ETA: 17s - loss: 0.8213 - acc: 0.64 - ETA: 17s - loss: 0.8222 - acc: 0.64 - ETA: 17s - loss: 0.8206 - acc: 0.64 - ETA: 17s - loss: 0.8216 - acc: 0.64 - ETA: 17s - loss: 0.8263 - acc: 0.63 - ETA: 16s - loss: 0.8250 - acc: 0.64 - ETA: 16s - loss: 0.8239 - acc: 0.64 - ETA: 16s - loss: 0.8246 - acc: 0.64 - ETA: 16s - loss: 0.8228 - acc: 0.64 - ETA: 16s - loss: 0.8229 - acc: 0.64 - ETA: 16s - loss: 0.8245 - acc: 0.64 - ETA: 15s - loss: 0.8234 - acc: 0.64 - ETA: 15s - loss: 0.8243 - acc: 0.64 - ETA: 15s - loss: 0.8215 - acc: 0.64 - ETA: 15s - loss: 0.8253 - acc: 0.64 - ETA: 15s - loss: 0.8256 - acc: 0.64 - ETA: 14s - loss: 0.8251 - acc: 0.64 - ETA: 14s - loss: 0.8250 - acc: 0.64 - ETA: 14s - loss: 0.8254 - acc: 0.64 - ETA: 14s - loss: 0.8213 - acc: 0.64 - ETA: 14s - loss: 0.8176 - acc: 0.65 - ETA: 14s - loss: 0.8146 - acc: 0.65 - ETA: 13s - loss: 0.8143 - acc: 0.65 - ETA: 13s - loss: 0.8145 - acc: 0.65 - ETA: 13s - loss: 0.8180 - acc: 0.65 - ETA: 13s - loss: 0.8169 - acc: 0.65 - ETA: 13s - loss: 0.8136 - acc: 0.65 - ETA: 12s - loss: 0.8124 - acc: 0.65 - ETA: 12s - loss: 0.8133 - acc: 0.65 - ETA: 12s - loss: 0.8156 - acc: 0.65 - ETA: 12s - loss: 0.8126 - acc: 0.65 - ETA: 12s - loss: 0.8137 - acc: 0.65 - ETA: 12s - loss: 0.8143 - acc: 0.65 - ETA: 11s - loss: 0.8118 - acc: 0.65 - ETA: 11s - loss: 0.8129 - acc: 0.65 - ETA: 11s - loss: 0.8108 - acc: 0.65 - ETA: 11s - loss: 0.8093 - acc: 0.65 - ETA: 11s - loss: 0.8086 - acc: 0.65 - ETA: 11s - loss: 0.8066 - acc: 0.65 - ETA: 10s - loss: 0.8061 - acc: 0.65 - ETA: 10s - loss: 0.8062 - acc: 0.65 - ETA: 10s - loss: 0.8048 - acc: 0.65 - ETA: 10s - loss: 0.8022 - acc: 0.65 - ETA: 10s - loss: 0.8002 - acc: 0.65 - ETA: 9s - loss: 0.7996 - acc: 0.6562 - ETA: 9s - loss: 0.7981 - acc: 0.656 - ETA: 9s - loss: 0.7976 - acc: 0.656 - ETA: 9s - loss: 0.7966 - acc: 0.657 - ETA: 9s - loss: 0.7941 - acc: 0.657 - ETA: 9s - loss: 0.7918 - acc: 0.659 - ETA: 8s - loss: 0.7921 - acc: 0.658 - ETA: 8s - loss: 0.7936 - acc: 0.657 - ETA: 8s - loss: 0.7922 - acc: 0.657 - ETA: 8s - loss: 0.7917 - acc: 0.657 - ETA: 8s - loss: 0.7928 - acc: 0.657 - ETA: 7s - loss: 0.7936 - acc: 0.656 - ETA: 7s - loss: 0.7914 - acc: 0.657 - ETA: 7s - loss: 0.7920 - acc: 0.658 - ETA: 7s - loss: 0.7904 - acc: 0.659 - ETA: 7s - loss: 0.7904 - acc: 0.660 - ETA: 7s - loss: 0.7896 - acc: 0.661 - ETA: 6s - loss: 0.7888 - acc: 0.661 - ETA: 6s - loss: 0.7890 - acc: 0.660 - ETA: 6s - loss: 0.7896 - acc: 0.659 - ETA: 6s - loss: 0.7883 - acc: 0.658 - ETA: 6s - loss: 0.7893 - acc: 0.657 - ETA: 6s - loss: 0.7894 - acc: 0.657 - ETA: 5s - loss: 0.7891 - acc: 0.657 - ETA: 5s - loss: 0.7913 - acc: 0.656 - ETA: 5s - loss: 0.7920 - acc: 0.656 - ETA: 5s - loss: 0.7906 - acc: 0.657 - ETA: 5s - loss: 0.7915 - acc: 0.656 - ETA: 4s - loss: 0.7910 - acc: 0.656 - ETA: 4s - loss: 0.7912 - acc: 0.656 - ETA: 4s - loss: 0.7931 - acc: 0.656 - ETA: 4s - loss: 0.7932 - acc: 0.656 - ETA: 4s - loss: 0.7921 - acc: 0.657 - ETA: 4s - loss: 0.7910 - acc: 0.657 - ETA: 3s - loss: 0.7904 - acc: 0.658 - ETA: 3s - loss: 0.7884 - acc: 0.659 - ETA: 3s - loss: 0.7875 - acc: 0.659 - ETA: 3s - loss: 0.7893 - acc: 0.658 - ETA: 3s - loss: 0.7874 - acc: 0.659 - ETA: 3s - loss: 0.7884 - acc: 0.660 - ETA: 2s - loss: 0.7874 - acc: 0.661 - ETA: 2s - loss: 0.7863 - acc: 0.661 - ETA: 2s - loss: 0.7856 - acc: 0.662 - ETA: 2s - loss: 0.7863 - acc: 0.662 - ETA: 2s - loss: 0.7866 - acc: 0.662 - ETA: 1s - loss: 0.7855 - acc: 0.662 - ETA: 1s - loss: 0.7853 - acc: 0.662 - ETA: 1s - loss: 0.7884 - acc: 0.661 - ETA: 1s - loss: 0.7872 - acc: 0.660 - ETA: 1s - loss: 0.7902 - acc: 0.660 - ETA: 1s - loss: 0.7924 - acc: 0.660 - ETA: 0s - loss: 0.7922 - acc: 0.660 - ETA: 0s - loss: 0.7927 - acc: 0.660 - ETA: 0s - loss: 0.7923 - acc: 0.660 - ETA: 0s - loss: 0.7916 - acc: 0.660 - ETA: 0s - loss: 0.7915 - acc: 0.660 - 31s 12ms/step - loss: 0.7904 - acc: 0.6616 - val_loss: 0.9921 - val_acc: 0.5893\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.55467 to 0.58933, saving model to BEST_WEIGHTSCNN0550554.5.10.2018.9.34.h5\n",
      "Epoch 4/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624/2719 [===========================>..] - ETA: 32s - loss: 0.5755 - acc: 0.75 - ETA: 32s - loss: 0.6917 - acc: 0.78 - ETA: 31s - loss: 0.8075 - acc: 0.70 - ETA: 30s - loss: 0.7928 - acc: 0.71 - ETA: 30s - loss: 0.7780 - acc: 0.70 - ETA: 30s - loss: 0.8201 - acc: 0.67 - ETA: 29s - loss: 0.7554 - acc: 0.71 - ETA: 29s - loss: 0.7046 - acc: 0.73 - ETA: 29s - loss: 0.7100 - acc: 0.73 - ETA: 29s - loss: 0.7014 - acc: 0.72 - ETA: 28s - loss: 0.7001 - acc: 0.72 - ETA: 28s - loss: 0.7058 - acc: 0.72 - ETA: 28s - loss: 0.6975 - acc: 0.72 - ETA: 28s - loss: 0.6898 - acc: 0.73 - ETA: 27s - loss: 0.6680 - acc: 0.74 - ETA: 27s - loss: 0.6725 - acc: 0.73 - ETA: 27s - loss: 0.6517 - acc: 0.74 - ETA: 27s - loss: 0.6534 - acc: 0.74 - ETA: 27s - loss: 0.6608 - acc: 0.74 - ETA: 26s - loss: 0.6690 - acc: 0.74 - ETA: 26s - loss: 0.6586 - acc: 0.74 - ETA: 26s - loss: 0.6549 - acc: 0.74 - ETA: 26s - loss: 0.6395 - acc: 0.75 - ETA: 26s - loss: 0.6389 - acc: 0.75 - ETA: 25s - loss: 0.6312 - acc: 0.75 - ETA: 25s - loss: 0.6222 - acc: 0.75 - ETA: 25s - loss: 0.6315 - acc: 0.75 - ETA: 25s - loss: 0.6321 - acc: 0.75 - ETA: 25s - loss: 0.6366 - acc: 0.75 - ETA: 24s - loss: 0.6260 - acc: 0.75 - ETA: 24s - loss: 0.6234 - acc: 0.75 - ETA: 24s - loss: 0.6198 - acc: 0.75 - ETA: 24s - loss: 0.6134 - acc: 0.75 - ETA: 24s - loss: 0.6139 - acc: 0.75 - ETA: 24s - loss: 0.6170 - acc: 0.75 - ETA: 23s - loss: 0.6140 - acc: 0.75 - ETA: 23s - loss: 0.6096 - acc: 0.75 - ETA: 23s - loss: 0.6138 - acc: 0.75 - ETA: 23s - loss: 0.6143 - acc: 0.75 - ETA: 23s - loss: 0.6141 - acc: 0.75 - ETA: 23s - loss: 0.6183 - acc: 0.75 - ETA: 22s - loss: 0.6109 - acc: 0.75 - ETA: 22s - loss: 0.6043 - acc: 0.76 - ETA: 22s - loss: 0.6041 - acc: 0.75 - ETA: 22s - loss: 0.5966 - acc: 0.76 - ETA: 22s - loss: 0.5980 - acc: 0.76 - ETA: 21s - loss: 0.6074 - acc: 0.76 - ETA: 21s - loss: 0.6104 - acc: 0.76 - ETA: 21s - loss: 0.6066 - acc: 0.76 - ETA: 21s - loss: 0.6129 - acc: 0.76 - ETA: 21s - loss: 0.6142 - acc: 0.75 - ETA: 21s - loss: 0.6152 - acc: 0.75 - ETA: 20s - loss: 0.6124 - acc: 0.76 - ETA: 20s - loss: 0.6129 - acc: 0.76 - ETA: 20s - loss: 0.6106 - acc: 0.76 - ETA: 20s - loss: 0.6129 - acc: 0.75 - ETA: 20s - loss: 0.6103 - acc: 0.75 - ETA: 19s - loss: 0.6080 - acc: 0.75 - ETA: 19s - loss: 0.6062 - acc: 0.75 - ETA: 19s - loss: 0.6130 - acc: 0.75 - ETA: 19s - loss: 0.6090 - acc: 0.75 - ETA: 19s - loss: 0.6074 - acc: 0.75 - ETA: 19s - loss: 0.6114 - acc: 0.75 - ETA: 18s - loss: 0.6073 - acc: 0.75 - ETA: 18s - loss: 0.6048 - acc: 0.75 - ETA: 18s - loss: 0.6058 - acc: 0.76 - ETA: 18s - loss: 0.6018 - acc: 0.76 - ETA: 18s - loss: 0.6016 - acc: 0.76 - ETA: 17s - loss: 0.6014 - acc: 0.76 - ETA: 17s - loss: 0.6026 - acc: 0.75 - ETA: 17s - loss: 0.6002 - acc: 0.75 - ETA: 17s - loss: 0.6023 - acc: 0.75 - ETA: 17s - loss: 0.6056 - acc: 0.75 - ETA: 17s - loss: 0.6058 - acc: 0.75 - ETA: 16s - loss: 0.6050 - acc: 0.75 - ETA: 16s - loss: 0.6069 - acc: 0.75 - ETA: 16s - loss: 0.6114 - acc: 0.75 - ETA: 16s - loss: 0.6102 - acc: 0.75 - ETA: 16s - loss: 0.6092 - acc: 0.75 - ETA: 16s - loss: 0.6082 - acc: 0.75 - ETA: 15s - loss: 0.6069 - acc: 0.75 - ETA: 15s - loss: 0.6049 - acc: 0.75 - ETA: 15s - loss: 0.6077 - acc: 0.74 - ETA: 15s - loss: 0.6069 - acc: 0.75 - ETA: 15s - loss: 0.6028 - acc: 0.75 - ETA: 14s - loss: 0.6050 - acc: 0.75 - ETA: 14s - loss: 0.6046 - acc: 0.75 - ETA: 14s - loss: 0.6050 - acc: 0.75 - ETA: 14s - loss: 0.6018 - acc: 0.75 - ETA: 14s - loss: 0.6022 - acc: 0.75 - ETA: 14s - loss: 0.6026 - acc: 0.75 - ETA: 13s - loss: 0.6067 - acc: 0.75 - ETA: 13s - loss: 0.6042 - acc: 0.75 - ETA: 13s - loss: 0.6020 - acc: 0.75 - ETA: 13s - loss: 0.6030 - acc: 0.75 - ETA: 13s - loss: 0.6056 - acc: 0.75 - ETA: 12s - loss: 0.6045 - acc: 0.75 - ETA: 12s - loss: 0.6060 - acc: 0.75 - ETA: 12s - loss: 0.6108 - acc: 0.75 - ETA: 12s - loss: 0.6077 - acc: 0.75 - ETA: 12s - loss: 0.6093 - acc: 0.75 - ETA: 12s - loss: 0.6056 - acc: 0.75 - ETA: 11s - loss: 0.6030 - acc: 0.75 - ETA: 11s - loss: 0.6003 - acc: 0.75 - ETA: 11s - loss: 0.6022 - acc: 0.75 - ETA: 11s - loss: 0.6016 - acc: 0.75 - ETA: 11s - loss: 0.6094 - acc: 0.75 - ETA: 11s - loss: 0.6111 - acc: 0.75 - ETA: 10s - loss: 0.6134 - acc: 0.75 - ETA: 10s - loss: 0.6166 - acc: 0.75 - ETA: 10s - loss: 0.6158 - acc: 0.75 - ETA: 10s - loss: 0.6143 - acc: 0.75 - ETA: 10s - loss: 0.6132 - acc: 0.75 - ETA: 9s - loss: 0.6126 - acc: 0.7544 - ETA: 9s - loss: 0.6119 - acc: 0.754 - ETA: 9s - loss: 0.6117 - acc: 0.754 - ETA: 9s - loss: 0.6111 - acc: 0.754 - ETA: 9s - loss: 0.6116 - acc: 0.755 - ETA: 9s - loss: 0.6109 - acc: 0.756 - ETA: 8s - loss: 0.6097 - acc: 0.756 - ETA: 8s - loss: 0.6089 - acc: 0.757 - ETA: 8s - loss: 0.6110 - acc: 0.757 - ETA: 8s - loss: 0.6156 - acc: 0.755 - ETA: 8s - loss: 0.6124 - acc: 0.756 - ETA: 7s - loss: 0.6121 - acc: 0.757 - ETA: 7s - loss: 0.6122 - acc: 0.756 - ETA: 7s - loss: 0.6123 - acc: 0.754 - ETA: 7s - loss: 0.6111 - acc: 0.755 - ETA: 7s - loss: 0.6124 - acc: 0.754 - ETA: 7s - loss: 0.6099 - acc: 0.755 - ETA: 6s - loss: 0.6093 - acc: 0.755 - ETA: 6s - loss: 0.6076 - acc: 0.756 - ETA: 6s - loss: 0.6070 - acc: 0.756 - ETA: 6s - loss: 0.6095 - acc: 0.755 - ETA: 6s - loss: 0.6081 - acc: 0.756 - ETA: 6s - loss: 0.6056 - acc: 0.756 - ETA: 5s - loss: 0.6035 - acc: 0.758 - ETA: 5s - loss: 0.6021 - acc: 0.758 - ETA: 5s - loss: 0.6009 - acc: 0.759 - ETA: 5s - loss: 0.6012 - acc: 0.759 - ETA: 5s - loss: 0.5993 - acc: 0.761 - ETA: 4s - loss: 0.5982 - acc: 0.761 - ETA: 4s - loss: 0.5986 - acc: 0.761 - ETA: 4s - loss: 0.5956 - acc: 0.762 - ETA: 4s - loss: 0.5943 - acc: 0.763 - ETA: 4s - loss: 0.5927 - acc: 0.765 - ETA: 4s - loss: 0.5917 - acc: 0.764 - ETA: 3s - loss: 0.5943 - acc: 0.763 - ETA: 3s - loss: 0.5952 - acc: 0.763 - ETA: 3s - loss: 0.5969 - acc: 0.762 - ETA: 3s - loss: 0.5969 - acc: 0.761 - ETA: 3s - loss: 0.5941 - acc: 0.763 - ETA: 3s - loss: 0.5924 - acc: 0.763 - ETA: 2s - loss: 0.5926 - acc: 0.763 - ETA: 2s - loss: 0.5918 - acc: 0.762 - ETA: 2s - loss: 0.5908 - acc: 0.762 - ETA: 2s - loss: 0.5899 - acc: 0.763 - ETA: 2s - loss: 0.5925 - acc: 0.762 - ETA: 1s - loss: 0.5917 - acc: 0.762 - ETA: 1s - loss: 0.5955 - acc: 0.761 - ETA: 1s - loss: 0.5983 - acc: 0.761 - ETA: 1s - loss: 0.5989 - acc: 0.761 - ETA: 1s - loss: 0.5988 - acc: 0.761 - ETA: 1s - loss: 0.5995 - acc: 0.7611"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-dca4040bab4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TRAINING...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodelDepths\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnConvLayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-78f872a5ee89>\u001b[0m in \u001b[0;36mfitModel\u001b[1;34m(model, modelName, Xtrain, Ytrain, Xval, Yval, epochs, batch_size)\u001b[0m\n\u001b[0;32m     15\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mYval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                         callbacks = callbacks)\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mtoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished training in {} min ({} h)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoc\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3600\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = []\n",
    "#modelDepths = []\n",
    "accuracies = []\n",
    "nConvLayers = 4   \n",
    "epochs = 150\n",
    "batch_size = 16\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    model = CNNModel((Xtrain.shape[1],Xtrain.shape[2]),nConvLayers)\n",
    "    opt = optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer = opt,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "    #model.summary()\n",
    "   \n",
    "    modelName = 'CNN055055'+ str(nConvLayers)\n",
    "    #saveModelTopology(model,modelName)\n",
    "    model.save(modelName+'CEO.h5')\n",
    "    \n",
    "    print('TRAINING...')\n",
    "\n",
    "    history = fitModel(model,modelName, Xtrain, Ytrain, Xval, Yval, epochs,batch_size)\n",
    "    histories.append(history.history)   \n",
    "    #modelDepths.append(nConvLayers)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     acc = evaluateModel(model, modelName, Xval, Yval)\n",
    "#     accuracies.append(acc)\n",
    "    \n",
    "    \n",
    "#     with open(\"hist.txt\", \"w\") as f:\n",
    "#         for h in histories:\n",
    "#             f.write(str(h) +\"\\n\")\n",
    "\n",
    "#     with open(\"hist.txt\", \"r\") as f:\n",
    "#         for line in f:\n",
    "#             histo.append(int(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkOutHistory(history):\n",
    "    \n",
    "\n",
    "    # plot Accuracy over Epochs\n",
    "    plt.plot(history['acc'])\n",
    "    plt.plot(history['val_acc'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train Acc','Val Acc'])\n",
    "    plt.title('Accuracy for {} over epochs'.format(modelName))\n",
    "    plt.show()\n",
    "    print(\"Max val accuracy: \", max(history['val_acc']))\n",
    "    print(\"Max train accuracy: \", max(history['acc']))\n",
    "\n",
    "    # plot Loss over Epochs\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss','Val Loss'])\n",
    "    plt.title('Loss for {} over epochs'.format(modelName))\n",
    "    plt.show()\n",
    "        \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in histories:\n",
    "    checkOutHistory(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
